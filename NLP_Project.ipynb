{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import string \n",
    "\n",
    "file = open(\"./Data/notario_utf8.txt\", \"r\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "    \n",
    "lines = []\n",
    "c=0\n",
    "for i in file:\n",
    "    c+=1\n",
    "    #print(c)\n",
    "    try:\n",
    "        lines.append(i)\n",
    "    except: \n",
    "        print(\"An error occured\") \n",
    "    \n",
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9724 samples\n",
      "Epoch 1/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 8.9974\n",
      "Epoch 00001: loss improved from inf to 8.99743, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 8.9974 - lr: 0.0100\n",
      "Epoch 2/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 8.7887\n",
      "Epoch 00002: loss improved from 8.99743 to 8.78867, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 8.7887 - lr: 0.0100\n",
      "Epoch 3/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 8.6337\n",
      "Epoch 00003: loss improved from 8.78867 to 8.63369, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 8.6337 - lr: 0.0100\n",
      "Epoch 4/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 8.4308\n",
      "Epoch 00004: loss improved from 8.63369 to 8.43080, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 8.4308 - lr: 0.0100\n",
      "Epoch 5/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 8.2163\n",
      "Epoch 00005: loss improved from 8.43080 to 8.21630, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 8.2163 - lr: 0.0100\n",
      "Epoch 6/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 8.0025\n",
      "Epoch 00006: loss improved from 8.21630 to 8.00246, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 8.0025 - lr: 0.0100\n",
      "Epoch 7/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 7.7759\n",
      "Epoch 00007: loss improved from 8.00246 to 7.77589, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 7.7759 - lr: 0.0100\n",
      "Epoch 8/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 7.5740\n",
      "Epoch 00008: loss improved from 7.77589 to 7.57399, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 46s 5ms/sample - loss: 7.5740 - lr: 0.0100\n",
      "Epoch 9/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 7.3726\n",
      "Epoch 00009: loss improved from 7.57399 to 7.37259, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 7.3726 - lr: 0.0100\n",
      "Epoch 10/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 7.1682\n",
      "Epoch 00010: loss improved from 7.37259 to 7.16824, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 7.1682 - lr: 0.0100\n",
      "Epoch 11/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 6.9643\n",
      "Epoch 00011: loss improved from 7.16824 to 6.96430, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 6.9643 - lr: 0.0100\n",
      "Epoch 12/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 6.7768\n",
      "Epoch 00012: loss improved from 6.96430 to 6.77682, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 6.7768 - lr: 0.0100\n",
      "Epoch 13/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 6.6069\n",
      "Epoch 00013: loss improved from 6.77682 to 6.60690, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 6.6069 - lr: 0.0100\n",
      "Epoch 14/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 6.4466\n",
      "Epoch 00014: loss improved from 6.60690 to 6.44658, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 6.4466 - lr: 0.0100\n",
      "Epoch 15/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 6.2816\n",
      "Epoch 00015: loss improved from 6.44658 to 6.28161, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 6.2816 - lr: 0.0100\n",
      "Epoch 16/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 6.1252\n",
      "Epoch 00016: loss improved from 6.28161 to 6.12522, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 6.1252 - lr: 0.0100\n",
      "Epoch 17/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.9899\n",
      "Epoch 00017: loss improved from 6.12522 to 5.98988, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.9899 - lr: 0.0100\n",
      "Epoch 18/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.8580\n",
      "Epoch 00018: loss improved from 5.98988 to 5.85805, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.8580 - lr: 0.0100\n",
      "Epoch 19/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.7402\n",
      "Epoch 00019: loss improved from 5.85805 to 5.74025, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.7402 - lr: 0.0100\n",
      "Epoch 20/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.6018\n",
      "Epoch 00020: loss improved from 5.74025 to 5.60185, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.6018 - lr: 0.0100\n",
      "Epoch 21/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.5059\n",
      "Epoch 00021: loss improved from 5.60185 to 5.50595, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.5059 - lr: 0.0100\n",
      "Epoch 22/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.3780\n",
      "Epoch 00022: loss improved from 5.50595 to 5.37797, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.3780 - lr: 0.0100\n",
      "Epoch 23/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.2648\n",
      "Epoch 00023: loss improved from 5.37797 to 5.26480, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.2648 - lr: 0.0100\n",
      "Epoch 24/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.1541\n",
      "Epoch 00024: loss improved from 5.26480 to 5.15414, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.1541 - lr: 0.0100\n",
      "Epoch 25/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 5.0394\n",
      "Epoch 00025: loss improved from 5.15414 to 5.03938, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 5.0394 - lr: 0.0100\n",
      "Epoch 26/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.9215\n",
      "Epoch 00026: loss improved from 5.03938 to 4.92145, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 4.9215 - lr: 0.0100\n",
      "Epoch 27/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.8192\n",
      "Epoch 00027: loss improved from 4.92145 to 4.81918, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 4.8192 - lr: 0.0100\n",
      "Epoch 28/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.7081\n",
      "Epoch 00028: loss improved from 4.81918 to 4.70814, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 4.7081 - lr: 0.0100\n",
      "Epoch 29/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.6272\n",
      "Epoch 00029: loss improved from 4.70814 to 4.62724, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 4.6272 - lr: 0.0100\n",
      "Epoch 30/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.5308\n",
      "Epoch 00030: loss improved from 4.62724 to 4.53081, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 4.5308 - lr: 0.0100\n",
      "Epoch 31/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.4386\n",
      "Epoch 00031: loss improved from 4.53081 to 4.43865, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 33s 3ms/sample - loss: 4.4386 - lr: 0.0100\n",
      "Epoch 32/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.3334\n",
      "Epoch 00032: loss improved from 4.43865 to 4.33343, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 33s 3ms/sample - loss: 4.3334 - lr: 0.0100\n",
      "Epoch 33/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.2399\n",
      "Epoch 00033: loss improved from 4.33343 to 4.23992, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 33s 3ms/sample - loss: 4.2399 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.1507\n",
      "Epoch 00034: loss improved from 4.23992 to 4.15067, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 4.1507 - lr: 0.0100\n",
      "Epoch 35/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 4.0488\n",
      "Epoch 00035: loss improved from 4.15067 to 4.04876, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 33s 3ms/sample - loss: 4.0488 - lr: 0.0100\n",
      "Epoch 36/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.9836\n",
      "Epoch 00036: loss improved from 4.04876 to 3.98357, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 3.9836 - lr: 0.0100\n",
      "Epoch 37/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.9071\n",
      "Epoch 00037: loss improved from 3.98357 to 3.90711, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.9071 - lr: 0.0100\n",
      "Epoch 38/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.8109\n",
      "Epoch 00038: loss improved from 3.90711 to 3.81093, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.8109 - lr: 0.0100\n",
      "Epoch 39/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.7311\n",
      "Epoch 00039: loss improved from 3.81093 to 3.73114, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.7311 - lr: 0.0100\n",
      "Epoch 40/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.6598\n",
      "Epoch 00040: loss improved from 3.73114 to 3.65979, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.6598 - lr: 0.0100\n",
      "Epoch 41/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.5786\n",
      "Epoch 00041: loss improved from 3.65979 to 3.57861, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.5786 - lr: 0.0100\n",
      "Epoch 42/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.5324\n",
      "Epoch 00042: loss improved from 3.57861 to 3.53237, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.5324 - lr: 0.0100\n",
      "Epoch 43/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.4428\n",
      "Epoch 00043: loss improved from 3.53237 to 3.44276, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.4428 - lr: 0.0100\n",
      "Epoch 44/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.3543\n",
      "Epoch 00044: loss improved from 3.44276 to 3.35429, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.3543 - lr: 0.0100\n",
      "Epoch 45/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.2929\n",
      "Epoch 00045: loss improved from 3.35429 to 3.29290, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 37s 4ms/sample - loss: 3.2929 - lr: 0.0100\n",
      "Epoch 46/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.2264\n",
      "Epoch 00046: loss improved from 3.29290 to 3.22645, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 3.2264 - lr: 0.0100\n",
      "Epoch 47/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.1618\n",
      "Epoch 00047: loss improved from 3.22645 to 3.16181, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.1618 - lr: 0.0100\n",
      "Epoch 48/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.1000\n",
      "Epoch 00048: loss improved from 3.16181 to 3.09999, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 3.1000 - lr: 0.0100\n",
      "Epoch 49/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 3.0267\n",
      "Epoch 00049: loss improved from 3.09999 to 3.02670, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 3.0267 - lr: 0.0100\n",
      "Epoch 50/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.9556\n",
      "Epoch 00050: loss improved from 3.02670 to 2.95556, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 36s 4ms/sample - loss: 2.9556 - lr: 0.0100\n",
      "Epoch 51/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.9009\n",
      "Epoch 00051: loss improved from 2.95556 to 2.90091, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 31s 3ms/sample - loss: 2.9009 - lr: 0.0100\n",
      "Epoch 52/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.8491\n",
      "Epoch 00052: loss improved from 2.90091 to 2.84907, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 2.8491 - lr: 0.0100\n",
      "Epoch 53/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.7835\n",
      "Epoch 00053: loss improved from 2.84907 to 2.78349, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 31s 3ms/sample - loss: 2.7835 - lr: 0.0100\n",
      "Epoch 54/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.7359\n",
      "Epoch 00054: loss improved from 2.78349 to 2.73586, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 35s 4ms/sample - loss: 2.7359 - lr: 0.0100\n",
      "Epoch 55/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.7053\n",
      "Epoch 00055: loss improved from 2.73586 to 2.70525, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 35s 4ms/sample - loss: 2.7053 - lr: 0.0100\n",
      "Epoch 56/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.6684\n",
      "Epoch 00056: loss improved from 2.70525 to 2.66840, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 2.6684 - lr: 0.0100\n",
      "Epoch 57/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.6046\n",
      "Epoch 00057: loss improved from 2.66840 to 2.60456, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 35s 4ms/sample - loss: 2.6046 - lr: 0.0100\n",
      "Epoch 58/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.5745\n",
      "Epoch 00058: loss improved from 2.60456 to 2.57446, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 37s 4ms/sample - loss: 2.5745 - lr: 0.0100\n",
      "Epoch 59/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.5203\n",
      "Epoch 00059: loss improved from 2.57446 to 2.52035, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 31s 3ms/sample - loss: 2.5203 - lr: 0.0100\n",
      "Epoch 60/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.4842\n",
      "Epoch 00060: loss improved from 2.52035 to 2.48423, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 2.4842 - lr: 0.0100\n",
      "Epoch 61/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.4352\n",
      "Epoch 00061: loss improved from 2.48423 to 2.43525, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 2.4352 - lr: 0.0100\n",
      "Epoch 62/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.3939\n",
      "Epoch 00062: loss improved from 2.43525 to 2.39393, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 37s 4ms/sample - loss: 2.3939 - lr: 0.0100\n",
      "Epoch 63/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.3553\n",
      "Epoch 00063: loss improved from 2.39393 to 2.35528, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 2.3553 - lr: 0.0100\n",
      "Epoch 64/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.3215\n",
      "Epoch 00064: loss improved from 2.35528 to 2.32147, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 2.3215 - lr: 0.0100\n",
      "Epoch 65/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.2924\n",
      "Epoch 00065: loss improved from 2.32147 to 2.29242, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 2.2924 - lr: 0.0100\n",
      "Epoch 66/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.2509\n",
      "Epoch 00066: loss improved from 2.29242 to 2.25085, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.2509 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.2889\n",
      "Epoch 00067: loss did not improve from 2.25085\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 2.2889 - lr: 0.0100\n",
      "Epoch 68/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.2113\n",
      "Epoch 00068: loss improved from 2.25085 to 2.21129, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 31s 3ms/sample - loss: 2.2113 - lr: 0.0100\n",
      "Epoch 69/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.1738\n",
      "Epoch 00069: loss improved from 2.21129 to 2.17378, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 2.1738 - lr: 0.0100\n",
      "Epoch 70/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.1579\n",
      "Epoch 00070: loss improved from 2.17378 to 2.15790, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.1579 - lr: 0.0100\n",
      "Epoch 71/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.1196\n",
      "Epoch 00071: loss improved from 2.15790 to 2.11963, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.1196 - lr: 0.0100\n",
      "Epoch 72/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.0766\n",
      "Epoch 00072: loss improved from 2.11963 to 2.07658, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.0766 - lr: 0.0100\n",
      "Epoch 73/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.0665\n",
      "Epoch 00073: loss improved from 2.07658 to 2.06645, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.0665 - lr: 0.0100\n",
      "Epoch 74/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.0641\n",
      "Epoch 00074: loss improved from 2.06645 to 2.06412, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.0641 - lr: 0.0100\n",
      "Epoch 75/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.0534\n",
      "Epoch 00075: loss improved from 2.06412 to 2.05337, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.0534 - lr: 0.0100\n",
      "Epoch 76/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 2.0074\n",
      "Epoch 00076: loss improved from 2.05337 to 2.00739, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 2.0074 - lr: 0.0100\n",
      "Epoch 77/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.9573\n",
      "Epoch 00077: loss improved from 2.00739 to 1.95734, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 1.9573 - lr: 0.0100\n",
      "Epoch 78/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.9189\n",
      "Epoch 00078: loss improved from 1.95734 to 1.91894, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 1.9189 - lr: 0.0100\n",
      "Epoch 79/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.9121\n",
      "Epoch 00079: loss improved from 1.91894 to 1.91215, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 1.9121 - lr: 0.0100\n",
      "Epoch 80/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.9275\n",
      "Epoch 00080: loss did not improve from 1.91215\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.9275 - lr: 0.0100\n",
      "Epoch 81/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.9534\n",
      "Epoch 00081: loss did not improve from 1.91215\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.9534 - lr: 0.0100\n",
      "Epoch 82/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.9318\n",
      "Epoch 00082: loss did not improve from 1.91215\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 1.9318 - lr: 0.0100\n",
      "Epoch 83/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.5090\n",
      "Epoch 00083: loss improved from 1.91215 to 1.50903, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.5090 - lr: 0.0020\n",
      "Epoch 84/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.3199\n",
      "Epoch 00084: loss improved from 1.50903 to 1.31990, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.3199 - lr: 0.0020\n",
      "Epoch 85/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.2578\n",
      "Epoch 00085: loss improved from 1.31990 to 1.25781, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.2578 - lr: 0.0020\n",
      "Epoch 86/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.2225\n",
      "Epoch 00086: loss improved from 1.25781 to 1.22249, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.2225 - lr: 0.0020\n",
      "Epoch 87/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.2003\n",
      "Epoch 00087: loss improved from 1.22249 to 1.20032, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.2003 - lr: 0.0020\n",
      "Epoch 88/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1797\n",
      "Epoch 00088: loss improved from 1.20032 to 1.17966, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1797 - lr: 0.0020\n",
      "Epoch 89/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1636\n",
      "Epoch 00089: loss improved from 1.17966 to 1.16363, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1636 - lr: 0.0020\n",
      "Epoch 90/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1493\n",
      "Epoch 00090: loss improved from 1.16363 to 1.14926, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1493 - lr: 0.0020\n",
      "Epoch 91/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1363\n",
      "Epoch 00091: loss improved from 1.14926 to 1.13632, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1363 - lr: 0.0020\n",
      "Epoch 92/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1235\n",
      "Epoch 00092: loss improved from 1.13632 to 1.12355, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1235 - lr: 0.0020\n",
      "Epoch 93/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1130\n",
      "Epoch 00093: loss improved from 1.12355 to 1.11304, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1130 - lr: 0.0020\n",
      "Epoch 94/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.1019\n",
      "Epoch 00094: loss improved from 1.11304 to 1.10189, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.1019 - lr: 0.0020\n",
      "Epoch 95/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0927\n",
      "Epoch 00095: loss improved from 1.10189 to 1.09270, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0927 - lr: 0.0020\n",
      "Epoch 96/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0821\n",
      "Epoch 00096: loss improved from 1.09270 to 1.08212, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0821 - lr: 0.0020\n",
      "Epoch 97/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0710\n",
      "Epoch 00097: loss improved from 1.08212 to 1.07100, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0710 - lr: 0.0020\n",
      "Epoch 98/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0614\n",
      "Epoch 00098: loss improved from 1.07100 to 1.06139, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0614 - lr: 0.0020\n",
      "Epoch 99/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0541\n",
      "Epoch 00099: loss improved from 1.06139 to 1.05410, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 37s 4ms/sample - loss: 1.0541 - lr: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0433\n",
      "Epoch 00100: loss improved from 1.05410 to 1.04327, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0433 - lr: 0.0020\n",
      "Epoch 101/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0372\n",
      "Epoch 00101: loss improved from 1.04327 to 1.03723, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 1.0372 - lr: 0.0020\n",
      "Epoch 102/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0260\n",
      "Epoch 00102: loss improved from 1.03723 to 1.02604, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0260 - lr: 0.0020\n",
      "Epoch 103/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0204\n",
      "Epoch 00103: loss improved from 1.02604 to 1.02044, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0204 - lr: 0.0020\n",
      "Epoch 104/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0104\n",
      "Epoch 00104: loss improved from 1.02044 to 1.01038, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0104 - lr: 0.0020\n",
      "Epoch 105/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 1.0028\n",
      "Epoch 00105: loss improved from 1.01038 to 1.00283, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 1.0028 - lr: 0.0020\n",
      "Epoch 106/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9936\n",
      "Epoch 00106: loss improved from 1.00283 to 0.99356, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9936 - lr: 0.0020\n",
      "Epoch 107/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9850\n",
      "Epoch 00107: loss improved from 0.99356 to 0.98504, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9850 - lr: 0.0020\n",
      "Epoch 108/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9760\n",
      "Epoch 00108: loss improved from 0.98504 to 0.97598, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9760 - lr: 0.0020\n",
      "Epoch 109/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9668\n",
      "Epoch 00109: loss improved from 0.97598 to 0.96677, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9668 - lr: 0.0020\n",
      "Epoch 110/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9614\n",
      "Epoch 00110: loss improved from 0.96677 to 0.96141, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 0.9614 - lr: 0.0020\n",
      "Epoch 111/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9542\n",
      "Epoch 00111: loss improved from 0.96141 to 0.95421, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9542 - lr: 0.0020\n",
      "Epoch 112/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9487\n",
      "Epoch 00112: loss improved from 0.95421 to 0.94869, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9487 - lr: 0.0020\n",
      "Epoch 113/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9402\n",
      "Epoch 00113: loss improved from 0.94869 to 0.94022, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9402 - lr: 0.0020\n",
      "Epoch 114/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9305\n",
      "Epoch 00114: loss improved from 0.94022 to 0.93047, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 0.9305 - lr: 0.0020\n",
      "Epoch 115/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9300\n",
      "Epoch 00115: loss improved from 0.93047 to 0.93000, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9300 - lr: 0.0020\n",
      "Epoch 116/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9177\n",
      "Epoch 00116: loss improved from 0.93000 to 0.91775, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9177 - lr: 0.0020\n",
      "Epoch 117/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9055\n",
      "Epoch 00117: loss improved from 0.91775 to 0.90554, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9055 - lr: 0.0020\n",
      "Epoch 118/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.9014\n",
      "Epoch 00118: loss improved from 0.90554 to 0.90141, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 28s 3ms/sample - loss: 0.9014 - lr: 0.0020\n",
      "Epoch 119/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8905\n",
      "Epoch 00119: loss improved from 0.90141 to 0.89049, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 0.8905 - lr: 0.0020\n",
      "Epoch 120/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8931\n",
      "Epoch 00120: loss did not improve from 0.89049\n",
      "9724/9724 [==============================] - 27s 3ms/sample - loss: 0.8931 - lr: 0.0020\n",
      "Epoch 121/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8833\n",
      "Epoch 00121: loss improved from 0.89049 to 0.88329, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 29s 3ms/sample - loss: 0.8833 - lr: 0.0020\n",
      "Epoch 122/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8721\n",
      "Epoch 00122: loss improved from 0.88329 to 0.87205, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 30s 3ms/sample - loss: 0.8721 - lr: 0.0020\n",
      "Epoch 123/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8681\n",
      "Epoch 00123: loss improved from 0.87205 to 0.86812, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8681 - lr: 0.0020\n",
      "Epoch 124/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8604\n",
      "Epoch 00124: loss improved from 0.86812 to 0.86039, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8604 - lr: 0.0020\n",
      "Epoch 125/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8544\n",
      "Epoch 00125: loss improved from 0.86039 to 0.85439, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 42s 4ms/sample - loss: 0.8544 - lr: 0.0020\n",
      "Epoch 126/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8483\n",
      "Epoch 00126: loss improved from 0.85439 to 0.84831, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8483 - lr: 0.0020\n",
      "Epoch 127/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8453\n",
      "Epoch 00127: loss improved from 0.84831 to 0.84531, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8453 - lr: 0.0020\n",
      "Epoch 128/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8417\n",
      "Epoch 00128: loss improved from 0.84531 to 0.84167, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8417 - lr: 0.0020\n",
      "Epoch 129/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8322\n",
      "Epoch 00129: loss improved from 0.84167 to 0.83224, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8322 - lr: 0.0020\n",
      "Epoch 130/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8306\n",
      "Epoch 00130: loss improved from 0.83224 to 0.83064, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8306 - lr: 0.0020\n",
      "Epoch 131/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8217\n",
      "Epoch 00131: loss improved from 0.83064 to 0.82174, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8217 - lr: 0.0020\n",
      "Epoch 132/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8170\n",
      "Epoch 00132: loss improved from 0.82174 to 0.81700, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8170 - lr: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8105\n",
      "Epoch 00133: loss improved from 0.81700 to 0.81048, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8105 - lr: 0.0020\n",
      "Epoch 134/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8081\n",
      "Epoch 00134: loss improved from 0.81048 to 0.80807, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8081 - lr: 0.0020\n",
      "Epoch 135/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8033\n",
      "Epoch 00135: loss improved from 0.80807 to 0.80330, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 40s 4ms/sample - loss: 0.8033 - lr: 0.0020\n",
      "Epoch 136/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.8007\n",
      "Epoch 00136: loss improved from 0.80330 to 0.80070, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.8007 - lr: 0.0020\n",
      "Epoch 137/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7909\n",
      "Epoch 00137: loss improved from 0.80070 to 0.79089, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7909 - lr: 0.0020\n",
      "Epoch 138/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7889\n",
      "Epoch 00138: loss improved from 0.79089 to 0.78892, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7889 - lr: 0.0020\n",
      "Epoch 139/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7847\n",
      "Epoch 00139: loss improved from 0.78892 to 0.78474, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7847 - lr: 0.0020\n",
      "Epoch 140/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7810\n",
      "Epoch 00140: loss improved from 0.78474 to 0.78104, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7810 - lr: 0.0020\n",
      "Epoch 141/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7743\n",
      "Epoch 00141: loss improved from 0.78104 to 0.77429, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7743 - lr: 0.0020\n",
      "Epoch 142/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7728\n",
      "Epoch 00142: loss improved from 0.77429 to 0.77279, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7728 - lr: 0.0020\n",
      "Epoch 143/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7663\n",
      "Epoch 00143: loss improved from 0.77279 to 0.76631, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7663 - lr: 0.0020\n",
      "Epoch 144/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7653\n",
      "Epoch 00144: loss improved from 0.76631 to 0.76526, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7653 - lr: 0.0020\n",
      "Epoch 145/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7637\n",
      "Epoch 00145: loss improved from 0.76526 to 0.76374, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7637 - lr: 0.0020\n",
      "Epoch 146/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7632\n",
      "Epoch 00146: loss improved from 0.76374 to 0.76324, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7632 - lr: 0.0020\n",
      "Epoch 147/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7659\n",
      "Epoch 00147: loss did not improve from 0.76324\n",
      "9724/9724 [==============================] - 31s 3ms/sample - loss: 0.7659 - lr: 0.0020\n",
      "Epoch 148/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7627\n",
      "Epoch 00148: loss improved from 0.76324 to 0.76275, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7627 - lr: 0.0020\n",
      "Epoch 149/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7528\n",
      "Epoch 00149: loss improved from 0.76275 to 0.75278, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7528 - lr: 0.0020\n",
      "Epoch 150/150\n",
      "9724/9724 [==============================] - ETA: 0s - loss: 0.7485\n",
      "Epoch 00150: loss improved from 0.75278 to 0.74855, saving model to nextword1.h5\n",
      "9724/9724 [==============================] - 32s 3ms/sample - loss: 0.7485 - lr: 0.0020\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.01))\n",
    "history=model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image \n",
    "#pil_img = Image(filename='graph1.png')\n",
    "#display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "#plt.plot(history.history['accuracy'], label='accuracy')\n",
    "\n",
    "plt.title('Prdida')\n",
    "plt.xlabel('pocas')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
